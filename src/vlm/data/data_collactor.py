import torch

from dataclasses import dataclass
from typing import Any, Dict, List
from transformers import Qwen2_5_VLProcessor


# https://github.com/zhangfaen/finetune-Qwen2-VL/blob/main/finetune.py
# https://github.com/roboflow/notebooks/blob/main/notebooks/how-to-finetune-qwen2-5-vl-for-json-data-extraction.ipynb
def create_message_template(batch):
    return [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": batch["image"]},
                {
                    "type": "text",
                    "text": (
                        "Given the following image of a skin flap, classify it into one of these categories:\n"
                        "0 - Viable skin\n"
                        "1 - Skin with Venous Problems\n"
                        "2 - Skin with Arterial Problems\n"
                        "3 - Temporary Hypoperfused skin, yet viable\n"
                        "4 - Necrotic skin\n"
                        "5 - Scarred skin\n\n"
                        "Format your response as:\n"
                        "Classification: X\n"
                        "Explanation: <explanation here>"
                    ),
                },
            ],
        },
        {
            "role": "assistant",
            "content": [
                {
                    "type": "text",
                    "text": batch["text"],
                }
            ],
        },
    ]

def find_assistant_content_sublist_indexes(label):
    """
    A message from train_data/data.json may look like below:
        {
            "messages": [
                {'role': 'user', 'content': [{'type': 'image', 'image': 'train_data/1.jpeg'}, \
                    {'type': 'text', 'text': 'æè¿°ä¸€ä¸‹è¿™ä¸ªå›¾ç‰‡'}]},
                {'role': 'assistant', 'content': [{'type': 'text', 'text': \
                    'è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä½å¹´è½»å¥³å­å’Œå¥¹çš„ç‹—åœ¨æµ·æ»©ä¸Šç©è€çš„åœºæ™¯ã€‚å¥³å­ç©¿ç€æ ¼å­è¡¬è¡«å’Œé»‘è‰²è£¤å­ï¼Œ\
                        ååœ¨æ²™æ»©ä¸Šï¼Œä¸å¥¹çš„é‡‘æ¯›çŠ¬äº’åŠ¨ã€‚å¥¹ä»¬çš„æ‰‹è‡‚ä¼¸å±•ç€ï¼Œä¼¼ä¹åœ¨è¿›è¡ŒæŸç§æ¸¸æˆæˆ–è®­ç»ƒã€‚èƒŒæ™¯æ˜¯å¹¿\
                            é˜”çš„æµ·æ´‹å’Œæ™´æœ—çš„å¤©ç©ºï¼Œé˜³å…‰æ´’åœ¨æ²™æ»©ä¸Šï¼Œè¥é€ å‡ºæ¸©æš–è€Œå®é™çš„æ°›å›´ã€‚æ•´ä½“ç”»é¢å……æ»¡äº†å¿«ä¹å’Œæ”¾æ¾çš„æ„Ÿè§‰ã€‚'}]}
            ]
        }
    After apply_chat_template, the text will look like below:
        ['<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|>\
            <|vision_end|>æè¿°ä¸€ä¸‹è¿™ä¸ªå›¾ç‰‡<|im_end|>\n<|im_start|>assistant\nè¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä½å¹´è½»å¥³å­å’Œå¥¹çš„ç‹—åœ¨æµ·æ»©ä¸Šç©è€çš„åœº\
                æ™¯ã€‚å¥³å­ç©¿ç€æ ¼å­è¡¬è¡«å’Œé»‘è‰²è£¤å­ï¼Œååœ¨æ²™æ»©ä¸Šï¼Œä¸å¥¹çš„é‡‘æ¯›çŠ¬äº’åŠ¨ã€‚å¥¹ä»¬çš„æ‰‹è‡‚ä¼¸å±•ç€ï¼Œä¼¼ä¹åœ¨è¿›è¡ŒæŸç§æ¸¸æˆæˆ–è®­ç»ƒã€‚èƒŒæ™¯æ˜¯å¹¿é˜”çš„æµ·æ´‹\
                    å’Œæ™´æœ—çš„å¤©ç©ºï¼Œé˜³å…‰æ´’åœ¨æ²™æ»©ä¸Šï¼Œè¥é€ å‡ºæ¸©æš–è€Œå®é™çš„æ°›å›´ã€‚æ•´ä½“ç”»é¢å……æ»¡äº†å¿«ä¹å’Œæ”¾æ¾çš„æ„Ÿè§‰ã€‚<|im_end|>\n']

    This function tries to find the indexes of the assistant content in the input_ids list to build labels.
    """
    # (Pdb++) processor.tokenizer.encode("<|im_start|>assistant\n")
    # [151644, 77091, 198]
    # (Pdb++) processor.tokenizer.encode("<|im_end|>\n")
    # [151645, 198]

    start_indexes = []
    end_indexes = []

    # Iterate through the list to find starting points
    for i in range(len(label) - 2):
        # Check if the current and next elements form the start sequence
        if label[i] == 151644 and label[i + 1] == 77091 and label[i + 2] == 198:
            start_indexes.append(i + 3)
            # Now look for the first 151645 and 198 after the start
            for j in range(i + 3, len(label) - 1):
                if label[j] == 151645 and label[j + 1] == 198:
                    end_indexes.append(j + 2)
                    # **NOTE** the <|im_end|>\n 2 tokens should be included in the label, so that model \can predicate end of output.
                    break  # Move to the next start after finding the end

    return list(zip(start_indexes, end_indexes))


@dataclass
class DataCollatorForQwenVL:
    processor: Any

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        text, image_inputs, video_inputs = map(
            list,
            zip(
                *[
                    (
                        self.processor.apply_chat_template(
                            create_message_template(batch),
                            tokenize=False,
                            add_generation_prompt=False,
                        ),
                        # batch["text"],
                        batch["image_inputs"],
                        batch["video_inputs"],
                    )
                    for batch in features
                ]
            ),
        )
        batch = self.processor(
            text=text,
            images=image_inputs if image_inputs[0] is not None else None,
            videos=video_inputs if video_inputs[0] is not None else None,
            padding=True,
            return_tensors="pt",
        )
        # labels = batch["input_ids"].clone()
        # if self.processor.tokenizer.pad_token_id is not None:
        #     labels[labels == self.processor.tokenizer.pad_token_id] = -100
        # batch["labels"] = labels

        labels_list = batch["input_ids"].clone()  # .tolist()
        labels_list[labels_list == self.processor.tokenizer.pad_token_id] = -100

        if isinstance(self.processor, Qwen2_5_VLProcessor):
            image_tokens = [151652, 151653, 151655]
        else:
            image_tokens = [
                self.processor.tokenizer.convert_tokens_to_ids(
                    self.processor.image_token
                )
            ]

        # labels_list = []
        # for ids_list in input_ids_lists:
        # label_ids = [-100] * len(ids_list)
        # for begin_end_indexs in find_assistant_content_sublist_indexes(
        #     ids_list, image_tokens
        # ):
        #     label_ids[begin_end_indexs[0] : begin_end_indexs[1]] = ids_list[
        #         begin_end_indexs[0] : begin_end_indexs[1]
        #     ]
        for image_token_id in image_tokens:
            labels_list[labels_list == image_token_id] = -100

        batch["labels"] = torch.tensor(labels_list, dtype=torch.int64)

            # DEBUG: Print out the prompt for the first sample in this batch
        sample_messages = create_message_template(batch[0])
        print("ğŸš¨ DEBUG | Training prompt sample:", sample_messages)

        return batch